{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98f9cae-c593-4842-a35a-b19f338b9841",
   "metadata": {},
   "source": [
    "# Distributed Computing\n",
    "A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another. Distributed computing is a field of computer science that studies distributed systems [link](https://en.wikipedia.org/wiki/Distributed_computing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ffdde8-1f52-44bd-b1dd-19dd2cff7fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25 s ± 63.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "#The list command is to make a readable output\n",
    "#in python3 map and filter create generators\n",
    "list(map(square, range(10**7)))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda91d84-b4e2-4914-a001-1e32f144ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66 s ± 55.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "from functools import reduce\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "reduce(add, map(square, range(10**7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b02c66e7-c266-4ca7-a760-dc4eb9024de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 s ± 45.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "from functools import reduce\n",
    "\n",
    "reduce(lambda x, y: x + y, map(lambda x: x * x, range(10**7)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf81766-1fde-49aa-8eb5-8f8d1370f4f6",
   "metadata": {},
   "source": [
    "## Parallel Computing with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77bc314-0884-4f02-b271-9a6bed1ac56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local[*]\", \"temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a72fab-f1f3-4c50-940d-c9cea845724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile('./data/docs/')\n",
    "lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "     .map(lambda word: (word.lower(), 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y) \\\n",
    "     .sortByKey() \\\n",
    "     .saveAsTextFile(\"counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef281bf8-3d89-4606-82f7-d1f16b610634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 13342)\n",
      "('a!_', 1)\n",
      "('a)', 9)\n",
      "('a).', 1)\n",
      "('a,', 10)\n",
      "('a--e', 2)\n",
      "('a--p.', 1)\n",
      "('a--well,', 1)\n",
      "('a-t-il', 1)\n",
      "('a.', 93)\n",
      "grep: write error: Broken pipe\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! cat counts/part-00000 | grep \"^('[a-z]\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b5e5315-ed6a-4d5d-b1f8-f29af80b8660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d085092d-0940-4630-b8e8-780b305414cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[17] at RDD at PythonRDD.scala:53 \n",
      " 663351\n"
     ]
    }
   ],
   "source": [
    "print(lines.flatMap(lambda line: line.split(\" \")),'\\n',lines.flatMap(lambda line: line.split(\" \")).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c3ff0b-d1fb-4b4e-86fa-0b51b359cec2",
   "metadata": {},
   "source": [
    "### Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "482f39f0-ad46-48c2-9330-825e6784010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "          .map(lambda word: (word.lower(), 1)) \\\n",
    "          .reduceByKey(lambda x, y: x + y) \\\n",
    "          .toDF(['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a19638b1-14c0-458d-9965-cd9700b66bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a0a48bf-eef3-442c-8030-97de65bfd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|            count|\n",
      "+-------+-----------------+\n",
      "|  count|            74405|\n",
      "|   mean|8.915408910691486|\n",
      "| stddev|266.3647125527789|\n",
      "|    min|                1|\n",
      "|    max|            46691|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68896842-9032-4fcb-ac55-c0b2adfdc264",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b96dcec-c86e-4d7b-9321-73d3a4db5833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "| the|46691|\n",
      "|  of|24986|\n",
      "|    |34179|\n",
      "|  it| 5819|\n",
      "|  is| 7530|\n",
      "| and|18239|\n",
      "|that| 6452|\n",
      "|  to|12389|\n",
      "|   a|13342|\n",
      "|  in|12717|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('counts')\n",
    "sqlContext.sql('select * from counts where count > 5000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b48ee-dfe8-4fb9-959c-6202800059b0",
   "metadata": {},
   "source": [
    "### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "374a03b4-edf2-4755-8b92-c515a316c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import UserDefinedFunction as udf\n",
    "from pyspark.sql.types import ArrayType, IntegerType\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4166a138-6081-4406-b700-a29ee3463e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length = udf(lambda x: Vectors.dense(len(x)), VectorUDT())\n",
    "feat_df = df.withColumn(\"features\", word_length(\"word\")) \\\n",
    "            .withColumnRenamed(\"count\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "189c4aea-6c7c-49a8-95f2-0f492a9c33e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/23 16:55:09 WARN Instrumentation: [54e32a0a] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([-3.5131])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg = LinearRegression()\n",
    "model = linreg.fit(feat_df)\n",
    "model.coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f611ad5-5640-48e2-b6e8-e949cfd959c4",
   "metadata": {},
   "source": [
    "## Spark setup\n",
    "Calculate the number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff14cb62-776c-4335-bb2d-e4ce10d8992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('./data/docs/', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7b85013-6166-4833-a7c7-17beedc16803",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=lines.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78e6d26b-7d10-4a15-b9be-5f0d15fbd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines2 = lines.repartition(1)\n",
    "s2 = lines2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "473a0ca7-5b5a-42f9-92a1-231d8dd9a576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert s2 == lines2.getNumPartitions()\n",
    "\n",
    "lines3 = (lines2.map(lambda x : (x, 1))\n",
    "              .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]), \n",
    "                          numPartitions=10)\n",
    "            )\n",
    "s3 = lines3.getNumPartitions()\n",
    "\n",
    "print(s1, s2, s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c3608-4013-4c28-9828-2b1a264e7acb",
   "metadata": {},
   "source": [
    "### Spark Variables\n",
    "In spark variables are used as shared variables for parallel computing. There are two types of shared variables:\n",
    "- Broadcast\n",
    "- Accumulator\n",
    "\n",
    "The variables are sent to each node of cluster for parallel processing.\n",
    "\n",
    "- Broadcast variables: \n",
    "These variables are available in all executors and they are cached a head of time. These variables are only sent to executors once and they are available for all tasks in the executors. There are pros and cons. The pros are the distribution of variables among nodes which reduces the number of work to transfer data and reduces the time of shuffling data. The broadcast variables should not be used in large datasets with the size of GB and they should be implemented in MB size datasets. \n",
    "\n",
    "- Accumulator:\n",
    "Accumulators are used when are called through the workers. The important functions which are used in $Accumulator$ are:\n",
    "    - **sparkContext.accumulator()** which is used to define the accumulator variables\n",
    "    - **add()** to add and update the accumulator values\n",
    "    - **value** which is the main property of the accumulator \n",
    "    \n",
    "#### Broadcast Variable Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da7f12d-0ef6-4b12-a2a4-75ca0a9bd9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John', 'Jones', 'USA', 'Facebook'), ('Jack', 'Jackson', 'USA', 'TESLA'), ('Jeniffer', 'Fred', 'USA', 'Google'), ('Jasmin', 'Jake', 'USA', 'Microsoft')]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('employees').getOrCreate()\n",
    "\n",
    "companies = {\"FB\":\"Facebook\", \"TSLA\":\"TESLA\", \"GOOGL\":\"Google\", \"MSFT\":\"Microsoft\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"John\",\"Jones\",\"USA\",\"FB\"),\n",
    "    (\"Jack\",\"Jackson\",\"USA\",\"TSLA\"),\n",
    "    (\"Jeniffer\",\"Fred\",\"USA\",\"GOOGL\"),\n",
    "    (\"Jasmin\",\"Jake\",\"USA\",\"MSFT\")\n",
    "  ]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "def employee_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = rdd.map(lambda x: (x[0],x[1],x[2], employee_convert(x[3]))).collect()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f3f50b0-857d-414c-9ada-d1e4eb11b92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First_Name: string (nullable = true)\n",
      " |-- Last_Name: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Company_Name: string (nullable = true)\n",
      "\n",
      "+----------+---------+-------+------------+\n",
      "|First_Name|Last_Name|Country|Company_Name|\n",
      "+----------+---------+-------+------------+\n",
      "|John      |Jones    |USA    |FB          |\n",
      "|Jack      |Jackson  |USA    |TSLA        |\n",
      "|Jeniffer  |Fred     |USA    |GOOGL       |\n",
      "|Jasmin    |Jake     |USA    |MSFT        |\n",
      "+----------+---------+-------+------------+\n",
      "\n",
      "+----------+---------+-------+------------+\n",
      "|First_Name|Last_Name|Country|Company_Name|\n",
      "+----------+---------+-------+------------+\n",
      "|John      |Jones    |USA    |Facebook    |\n",
      "|Jack      |Jackson  |USA    |TESLA       |\n",
      "|Jeniffer  |Fred     |USA    |Google      |\n",
      "|Jasmin    |Jake     |USA    |Microsoft   |\n",
      "+----------+---------+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('employee').getOrCreate()\n",
    "\n",
    "companies = {\"FB\":\"Facebook\", \"TSLA\":\"TESLA\", \"GOOGL\":\"Google\", \"MSFT\":\"Microsoft\"}\n",
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "data = [(\"John\",\"Jones\",\"USA\",\"FB\"),\n",
    "    (\"Jack\",\"Jackson\",\"USA\",\"TSLA\"),\n",
    "    (\"Jeniffer\",\"Fred\",\"USA\",\"GOOGL\"),\n",
    "    (\"Jasmin\",\"Jake\",\"USA\",\"MSFT\")\n",
    "  ]\n",
    "\n",
    "columns = [\"First_Name\",\"Last_Name\",\"Country\",\"Company_Name\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcastStates.value[code]\n",
    "\n",
    "result = df.rdd.map(lambda x: (x[0],x[1],x[2], employee_convert(x[3]))).toDF(columns)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd34767-0e61-43f6-bcf8-502c54331aef",
   "metadata": {},
   "source": [
    "#### Accumulator Variable Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19a79633-b897-45a2-a09e-0804c0461c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "sum of all variables defined in rdd:\n",
      " 200\n",
      "\n",
      "The number of variables defined in rdd2:\n",
      " 5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"salary_accumulator\").getOrCreate()\n",
    "\n",
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize([20, 30, 50, 100])\n",
    "\n",
    "print(accum.value) #Accessed by driver\n",
    "variables_sum=spark.sparkContext.accumulator(0)\n",
    "def summation(x):\n",
    "    global variables_sum\n",
    "    variables_sum+=x\n",
    "rdd.foreach(summation)\n",
    "print('\\nsum of all variables defined in rdd:\\n', variables_sum.value)\n",
    "\n",
    "\n",
    "variable_count=spark.sparkContext.accumulator(0)\n",
    "rdd2=spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "rdd2.foreach(lambda x:variable_count.add(1))\n",
    "print(\"\\nThe number of variables defined in rdd2:\\n\", variable_count.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
